{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TE2ItlsI956"
      },
      "source": [
        "# Séance 1 :  Deep Learning - Introduction à Pytorch \n",
        "\n",
        "Les notebooks sont très largement inspirés des cours de **N. Baskiotis et B. Piwowarski**. Ils peuvent être complétés efficacement par les tutoriels *officiels* présents sur le site de pytorch:\n",
        "https://pytorch.org/tutorials/\n",
        "\n",
        "Au niveau de la configuration, toutes les installations doivent fonctionner sur Linux et Mac. Pour windows, ça peut marcher avec Anaconda à jour... Mais il est difficile de récupérer les problèmes.\n",
        "\n",
        "* Aide à la configuration des machines: [lien](https://dac.lip6.fr/master/environnement-deep/)\n",
        "* Alternative 1 à Windows: installer Ubuntu sous Windows:  [Ubuntu WSL](https://ubuntu.com/wsl)\n",
        "* Alternative 2: travailler sur Google Colab (il faut un compte gmail + prendre le temps de comprendre comment accéder à des fichers) [Colab](https://colab.research.google.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAER7frwJu9L"
      },
      "source": [
        "# A. Préambule\n",
        "\n",
        "Les lignes suivantes permettent d'importer pytorch et vérifier si un GPU est disponible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Y9YOOHHhJKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac85c0a-7915-4377-dd3c-28c76dac38bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La version de torch est :  1.12.1+cu113\n",
            "Le calcul GPU est disponible ?  True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"La version de torch est : \",torch.__version__)\n",
        "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "jKtJM93s-UVU",
        "outputId": "90fa4a6c-eb1f-4429-c674-5bb899031e77"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ffcb08c8c4eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchmetrics'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# pour le chargement des données MNIST (à la fin)\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2LqFo3wzwYP"
      },
      "source": [
        "## Syntaxe\n",
        "\n",
        "Le principal objet manipulé sous Pytorch est **torch.Tensor** qui correspond à un tenseur mathématique (généralisation de la notion de matrice en $n$-dimensions), très proche dans l'utilisation de **numpy.array**.   Cet objet est optimisé pour les calculs sur GPU ce qui implique quelques contraintes plus importantes que sous **numpy**. En particulier :\n",
        "* le type du tenseur manipulé est très important et les conversions ne sont pas automatique (**FloatTensor** de type **torch.float**, **DoubleTensor** de type **torch.double**,  **ByteTensor** de type **torch.byte**, **IntTensor** de type **torch.int**, **LongTensor** de type **torch.long**). Pour un tenseur **t** La conversion se fait très simplement en utilisant les fonctions : **t.double()**, **t.float()**, **t.long()** ...\n",
        "* la plupart des opérations ont une version *inplace*, c'est-à-dire qui modifie le tenseur plutôt que de renvoyer un nouveau tenseur; elles sont suffixées par **_** (**add_** par exemple).\n",
        "\n",
        "Voici ci-dessous quelques exemples d'opérations usuelles, n'hésitez pas à vous référez à la [documentation officielle](https://pytorch.org/docs/stable/tensors.html) pour la liste exhaustive des opérations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "VZxNfy1b1u43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc10195-8460-4d46-bee9-bc5725811748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [2., 3., 4.]])\n",
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "# Création de tenseurs et caractéristiques\n",
        "## Créer un tenseur à partir d'une liste\n",
        "print(torch.tensor([[1.,2.,3.],[2.,3,4.]])) \n",
        "## Créer un tenseur  tenseur rempli de 1 de taille 2x3x4\n",
        "print(torch.ones(2,3,4)) \n",
        "## tenseur de zéros de taille 2x3 de type float\n",
        "print(torch.zeros(2,3,dtype=torch.float))  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkOgZQho-UVW",
        "outputId": "72c8d7d6-a781-4ce2-8a74-12be0d14f3bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[12., 10., 12.],\n",
            "        [13., 10., 11.]])\n",
            "tensor([[1.1399, 0.8869, 0.7279],\n",
            "        [1.1574, 1.0094, 1.0603]])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## tirage uniforme entier entre 10 et 15, \n",
        "## remarquez l'utilisation du _ dans random pour l'opération inplace\n",
        "print(torch.zeros(2,3).random_(10,15)) \n",
        "## tirage suivant la loi normale\n",
        "a=torch.zeros(2,3).normal_(1,0.1)\n",
        "print(a)\n",
        "## equivalent à zeros(3,4).normal_\n",
        "b = torch.randn(3,4) \n",
        "## Création d'un vecteur\n",
        "c = torch.randn(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwA1oR8q-UVX",
        "outputId": "724dbf4c-313f-4d24-e543-b911143835e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.1399, 0.8869, 0.7279, 1.1399, 0.8869, 0.7279],\n",
            "        [1.1574, 1.0094, 1.0603, 1.1574, 1.0094, 1.0603]])\n",
            "3 torch.Size([3, 4]) torch.Size([3])\n",
            "tensor([[1, 0, 0],\n",
            "        [1, 1, 1]], dtype=torch.int32) torch.IntTensor\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## concatenation de tenseurs\n",
        "print(torch.cat((a,a),1))\n",
        "## Taille des tenseurs/vecteurs shape => size\n",
        "print(a.size(1),b.shape,c.size())\n",
        "## Conversion de type\n",
        "print(a.int(),a.int().type())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJGHmD0F-UVY",
        "outputId": "e5169aa1-4e71-41be-b140-3c984163f8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5.8394)\n",
            "tensor([[ 1.0784,  2.5456, -0.3891, -0.9133],\n",
            "        [ 1.1450,  2.9930, -0.5309, -0.4646]]) tensor([[ 1.0784,  2.5456, -0.3891, -0.9133],\n",
            "        [ 1.1450,  2.9930, -0.5309, -0.4646]])\n",
            "tensor([[1.1399, 1.1574],\n",
            "        [0.8869, 1.0094],\n",
            "        [0.7279, 1.0603]]) tensor([[1.1399, 1.1574],\n",
            "        [0.8869, 1.0094],\n",
            "        [0.7279, 1.0603]])\n",
            "argmax :  tensor([0, 0])\n",
            "tensor([ 1.7298, -2.2343,  3.2028]) tensor(2.6984)\n",
            "tensor([ 0.4325, -0.5586,  0.8007]) tensor(0.2249)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Opérations élémentaires sur les tenseurs \n",
        "## produit scalaire (et contrairement à numpy, que produit scalaire)\n",
        "print(c.dot(c))\n",
        "## produit matriciel : utilisation de @ ou de la fonction mm\n",
        "print(a.mm(b), a @ b)\n",
        "## transposé\n",
        "print(a.t(),a.T)\n",
        "## index du maximum selon une dimension\n",
        "print(\"argmax : \",a.argmax(dim=1))\n",
        "## somme selon une dimension/de tous les éléments\n",
        "print(b.sum(1), b.sum()) \n",
        "## moyenne selon  une dimension/sur tous les éléments\n",
        "print(b.mean(1), b.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "z37nAbLH-UVa",
        "outputId": "bfcc6a8b-c503-46ea-a692-e587cb09bcee"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5bd7c8afd2d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ce qui marchait en numpy ne marche plus en torch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# ERREUR de type (même résultat avec n'importe quelle opération)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# OK pour un scalaire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# ERREUR de type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for @: 'Tensor' and 'list'"
          ]
        }
      ],
      "source": [
        "## ATTENTION: la spécificité et les capacités des tenseurs empêchent les conversions à la volée\n",
        "# ce qui marchait en numpy ne marche plus en torch\n",
        "\n",
        "print(a@[[1], [1], [1]])    # ERREUR de type (même résultat avec n'importe quelle opération)\n",
        "print(a*2)                  # OK pour un scalaire\n",
        "print(a*[2.,2.,2.])         # ERREUR de type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prWX_52c-UVa",
        "outputId": "6618428f-c7d4-4cf8-f781-e60996bd1620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.2799, 1.7738, 1.4558],\n",
            "        [2.3147, 2.0189, 2.1206]]) tensor([[1.2995, 0.7866, 0.5298],\n",
            "        [1.3395, 1.0190, 1.1242]]) tensor([[1.2995, 0.7866, 0.5298],\n",
            "        [1.3395, 1.0190, 1.1242]])\n",
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]]) tensor([[0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## somme/produit/puissance termes a termes\n",
        "print(a+a,a*a,a**2)\n",
        "## attention ! comme sous numpy, il peut y avoir des pièges ! \n",
        "## Vérifier toujours les dimensions !!\n",
        "a=torch.zeros(5,1)\n",
        "b = torch.zeros(5)\n",
        "## la première opération fait un broadcast et le résultat est tenseur à 2 dimensiosn,\n",
        "## le résultat de la deuxième opération est bien un vecteur\n",
        "print(a-b,a.t()-b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B6jwFWL-UVb",
        "outputId": "a18c9809-1620-4f52-e5c7-ceed655e8f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.9772,  0.8220, -0.2439, -0.8302],\n",
            "        [ 2.1531,  0.0099, -0.4101,  1.6129],\n",
            "        [-1.5572,  1.5220, -1.2244,  0.4714]])\n",
            "tensor([[-0.9772,  0.8220, -0.2439, -0.8302,  2.1531,  0.0099],\n",
            "        [-0.4101,  1.6129, -1.5572,  1.5220, -1.2244,  0.4714]])\n",
            "tensor([[-1.2292],\n",
            "        [ 3.3658],\n",
            "        [-0.7883]])\n",
            "tensor([-1.2292,  3.3658, -0.7883])\n",
            "tensor([[False, False, False],\n",
            "        [ True,  True,  True],\n",
            "        [False,  True, False]])\n",
            "tensor([False,  True, False])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## changer les dimensions du tenseur (la taille totale doit être inchangée) = np.reshape\n",
        "b = torch.randn(3,4) \n",
        "print(b)\n",
        "print(b.view(2,6))\n",
        "\n",
        "# on utilise souvent view pour passer de matrice à vecteur\n",
        "e=torch.tensor([[1],[1],[1],[1]], dtype=torch.float)\n",
        "d = b@e\n",
        "print(d)\n",
        "# conversion en vecteur\n",
        "print(d.view(-1))\n",
        "\n",
        "# usage typique\n",
        "y = torch.tensor([1,-1,1], dtype=torch.float)\n",
        "print(d > y)            # résultat catastrophique (dispatch)\n",
        "print(d.view(-1) > y)   # résultat attendu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzhX7D8KKIvt"
      },
      "source": [
        "# B. Autograd et graphe de calcul\n",
        "Un élément central de pytorch est le graphe de calcul : lors du calcul d'une variable, l'ensemble des opérations qui ont servies au calcul sont stockées sous la forme d'un graphe acyclique, dit de *calcul*. Les noeuds internes du graphe représentent les opérations, le noeud terminal le résultat et les racines les variables d'entrées. Ce graphe sert en particulier à calculer les dérivées partielles de la sortie par rapport aux variables d'entrées - en utilisant les règles de dérivations chainées des fonctions composées. \n",
        "Pour cela, toutes les fonctions disponibles dans pytorch comportent un mécanisme, appelé *autograd* (automatique differentiation), qui permet de calculer les dérivées partielles des opérations. \n",
        "\n",
        "## B.1. Différenciation automatique\n",
        "(De manière simplifiée, pour les détails cf [la documentation](https://pytorch.org/docs/stable/notes/extending.html))\n",
        "\n",
        "Toute opération sous pytorch hérite de la classe **Function** et doit définir :\n",
        "* une méthode **forward(\\*args)** : passe avant, calcule le résultat de la fonction appliquée aux arguments \n",
        "* une méthode **backward(\\*args)** : passe arrière, calcule les dérivées partielles par rapport aux entrées. Les arguments de  cette méthode correspondent aux valeurs des dérivées suivantes dans le graphe de calcul. En particulier, il y a autant d'arguments à **backward**  que de sorties pour la méthode **forward** (rétro-propagation : on doit connaître les dérivés qui viennent  en aval du calcul) et autant de sorties que d'arguments dans la méthode **forward** (chaque sortie correspond à  une dérivée partielle par rapport à chaque entrée du module). Le calcul se fait sur les valeurs du dernier appel de **forward**. \n",
        "\n",
        "Par exemple, pour la fonction d'addition  **add(x,y)**, **add.forward(x,y)** renverra **x+y** (l'appel de la fonction est équivalent à l'appel de **forward**) et **add.backward(1)** renverra le couple **(1,1)** (la dérivée par rapport à x, et celle par rapport à y) .\n",
        "\n",
        "En pratique, ce ne sont pas les méthodes de ces fonctions qui sont utilisées, mais des méthodes équivalentes sur les tenseurs. La méthode **backward** d'un tenseur permet de rétro-propager le calcul du gradient sur toutes les variables qui ont servies à son calcul.\n",
        "\n",
        "La valeur du gradient pour chaque dérivée partielle se trouve dans l'attribut **grad** de la variable concernée. \n",
        "\n",
        "Comme c'est un mécanisme lourd, l'autograd n'est pas activé par défaut pour une variable. Afin de l'activer, il faut mettre le flag **requires_grad** de cette variable à vrai. Dès lors, tout calcul qui utilise cette variable sera enregistré dans le graphe de calcul et le gradient sera disponible.\n",
        "\n",
        "\n",
        "Exemple : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_mYVeXMfsTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2112fb-8279-4c17-820a-e215888cf531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graphe de calcul ?  False\n",
            "Dérivée de z/a :  2.0  z/b : 1.0\n",
            "Erreur :  element 0 of tensors does not require grad and does not have a grad_fn\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor(1.)\n",
        "# Par défaut, requires_grad est à False\n",
        "print(\"Graphe de calcul ? \",a.requires_grad)\n",
        "# On peut demander à ce que le graphe de calcul soit retenu\n",
        "a.requires_grad = True \n",
        "# Ou lors de la création du tenseur directement\n",
        "b = torch.tensor(2.,requires_grad=True)\n",
        "z = 2*a + b\n",
        "# Calcul des dérivées partielles par rapport à z\n",
        "z.backward()\n",
        "print(\"Dérivée de z/a : \", a.grad.item(),\" z/b :\", b.grad.item())\n",
        "\n",
        "# Si on a oublié de demander le graphe de calcul :\n",
        "a, b = torch.tensor(1.),torch.tensor(2.)\n",
        "z = 2*a+b\n",
        "try: # on sait que ça va provoquer une erreur\n",
        "  z.backward()\n",
        "except Exception as e: # erreur => simple message\n",
        "  print(\"Erreur : \", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eApYmHIZa917"
      },
      "source": [
        "## B.2. <span class=\"alert-success\">     Exercice :  Utilisation de backward     </div>\n",
        "* Implémentez (en une ligne) la fonction de coût aux moindres carrés $MSE(\\hat{y},y)=\\frac{1}{2N} \\sum_{i=1}^N\\|\\hat{y_i}-y_i\\|^2$ où $\\hat{y},y$ sont deux matrices de taille $N\\times d$, et $y_i,\\hat{y_i}$ les $i$-èmes vecteurs lignes des matrices.\n",
        "* Engendrez **y,yhat** deux matrices aléatoires de taille $(1,5)$.\n",
        "* Calculez **MSE(y,yhat)**\n",
        "* Calculez à la main le gradient de **MSE** par rapport à **y**, **yhat**\n",
        "* Calculez grâce à pytorch le gradient de **MSE** par rapport à **y** et **yhat** et vérifier le résultat.\n",
        "* Appelez une deuxième fois **MSE** sur les mêmes vecteurs et la méthode **backward**. Qu'observez vous pour le gradient ? Comment l'expliquez vous ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Rd37M_3gkqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "955ffe71-e7de-4211-9cea-1eef505c7102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE : tensor(2.3437, grad_fn=<MeanBackward0>)\n",
            "tensor([[ 0.9240,  0.2294,  0.3007,  0.6209, -0.7019]]) tensor([[ 1.2379, -0.6195,  0.2026,  1.7525, -0.2125]])\n",
            "tensor([[-0.9240, -0.2294, -0.3007, -0.6209,  0.7019]]) tensor([[-1.0721, -1.1929, -0.5492,  0.2002,  1.5422]])\n"
          ]
        }
      ],
      "source": [
        "MSE =()\n",
        "def MSE(yhat,y): return ((y_hat-y)**2).mean()\n",
        "\n",
        "y = torch.randn(1,5,requires_grad=True)\n",
        "y_hat = torch.randn(1,5,requires_grad=True)\n",
        "mse = MSE(yhat,y)\n",
        "print(\"MSE :\" ,mse)\n",
        "\n",
        "### gradient with pytorch \n",
        "mse.backward()\n",
        "print(y.grad,y.data) ## gradient de MSE par rapport à Y \n",
        "print(y_hat.grad,y_hat.data)  \n",
        "\n",
        "# 1. retro-propager l'erreur\n",
        "# 2. afficher le gradient sur les deux vecteurs et comprendre ce qui se passe\n",
        "# 3. faire une itération supplémentaire et afficher de nouveau\n",
        "\n",
        "##  TODO \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxgCbApIgU5X"
      },
      "source": [
        "\n",
        "## B.3. <span class=\"alert-success\"> Exercice :   Régression linéaire en pytorch </span>\n",
        "\n",
        "* Définissez la fonction **flineaire(x,w,b)** fonction linéaire qui calcule $f(x,w,b)=x.w^t+b$  avec $x\\in \\mathbb{R}^{{n\\times d}},~w\\in\\mathbb{R}^{1,d}, b\\in \\mathbb{R}$\n",
        "* Complétez le code ci-dessous pour réaliser une descente de gradient et apprendre les paramètres optimaux de la regression linéaire : $$w^∗,b^∗=\\text{argmin}_{w,b}\\frac{1}{N} \\sum_{i=1}^N \\|f(x^i,w,b)-y^i\\|^2$$\n",
        "\n",
        "Pour tester votre code, utilisez le jeu de données très classique *Boston*, le prix des loyers à Boston en fonction de caractéristiques socio-économiques des quartiers. Le code ci-dessous permet de les charger.\n",
        "\n",
        "<span style=\"color:red\"> ATTENTION ! </span>\n",
        "* pour la mise-à-jour des paramètres, <span style=\"color:red\">vous ne pouvez pas faire directement</span> \n",
        "$$w = w-\\epsilon*gradient$$ \n",
        "(pourquoi ?). Vous devez passer par w.data qui permet de ne pas enregistrer les opérations dans le graphe de calcul (ou utiliser la méthode ```.detach()``` d'une variable qui permet de créer une copie détachée du graphe de calcul). \n",
        "* Note: il est aussi possible de faire:\n",
        "    ```\n",
        "    with torch.no_grad():\n",
        "        w -= eps*gradient\n",
        "    ```\n",
        "    * Désactivation temporaire du graph de calcul, on manipule les tensors comme des variables classiques\n",
        "    * ATTENTION à faire des ```-=``` ou ```+=``` => Si vous construisez un nouveau tenseur, il ne se reconnectera pas au graphe de calcul!\n",
        "* l'algorithme doit converger avec la valeur de epsilon fixée; si ce n'est pas le cas, il y a une erreur (la plupart du temps au niveau du calcul du coût).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dArOgSWNTVvb"
      },
      "outputs": [],
      "source": [
        "def flineaire(x,w,b):return (x@w.t()+b)\n",
        "\n",
        "## Chargement des données Boston (depuis sklearn) et transformation en tensor.\n",
        "from sklearn.datasets import load_boston\n",
        "boston = load_boston() ## chargement des données\n",
        "boston_x = torch.tensor(boston['data'],dtype=torch.float) # penser à typer les données pour éliminer les incertitudes\n",
        "boston_y = torch.tensor(boston['target'],dtype=torch.float)\n",
        "\n",
        "print(\"Nombre d'exemples : \",boston_x.size(0), \"Dimension : \",boston_x.size(1))\n",
        "print(\"Nom des attributs : \", \", \".join(boston['feature_names']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGhkUgM2-UVe"
      },
      "outputs": [],
      "source": [
        "\n",
        "EPOCHS = 5000\n",
        "EPS = 1e-6\n",
        "#initialisation aléatoire de w et b\n",
        "w = torch.randn(1,boston_x.size(1),requires_grad=True)\n",
        "b =  torch.randn(1,1,requires_grad=True)\n",
        "loss_h = [] # sauvegarde des valeurs de loss (pas si trivial!)\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "  y_hat = flineaire(boston_x,w,b)\n",
        "  mse = MSE(y_hat , boston_y)\n",
        "  mse.backward()\n",
        "  loss_h.append(mse.data)\n",
        "  with torch.no_grad(): ### descente de gradient \n",
        "      w -= EPS*w.grad \n",
        "      b -= EPS*b.grad\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()\n",
        "\n",
        "    ## SOLUTION 1: Penser à aller chercher w.data (et sa contrepartie dans le gradient)\n",
        "    # 1. Construire la loss (+stocker la valeur dans loss_h)\n",
        "    # 2. Retro-propager\n",
        "    # 3. MAJ des paramètres\n",
        "    # 4. Penser à remettre le gradient à 0 (cf exo précédent)\n",
        "    ##  TODO "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_arJRk9A-UVe"
      },
      "outputs": [],
      "source": [
        "# une seconde version du même code avec l'environnement torch.no_grad()\n",
        "# attention, dans ce cas, le += est obligatoire\n",
        "# code identique (juste changer les 2 lignes de MAJ)\n",
        "\n",
        "EPOCHS = 5000\n",
        "EPS = 1e-6\n",
        "#initialisation aléatoire de w et b\n",
        "w = torch.randn(1,boston_x.size(1),requires_grad=True)\n",
        "b =  torch.randn(1,1,requires_grad=True)\n",
        "loss_h = [] # sauvegarde des valeurs de loss (pas si trivial!)\n",
        "for i in range(EPOCHS):\n",
        "    pass\n",
        "    ## SOLUTION 2: avec torch.no_grad() [toutes les lignes sont identiques, sauf les 2 lignes de MAJ des paramètres]\n",
        "    ##  TODO "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "TdeGsZxr-UVe",
        "outputId": "e4052012-e29e-43c7-eccd-b2bfa4bbd1bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor(820770.3750), tensor(253927.2500), tensor(161997.9688), tensor(137644.9375), tensor(123796.0469), tensor(112373.2578), tensor(102159.1719), tensor(92903.7969), tensor(84499.8281), tensor(76866.4609)]\n",
            "[tensor(165.6208), tensor(165.6105), tensor(165.6003), tensor(165.5899), tensor(165.5796), tensor(165.5693), tensor(165.5590), tensor(165.5487), tensor(165.5385), tensor(165.5282)]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# affichage de l'optimisation\n",
        "plt.figure()\n",
        "plt.plot(loss_h)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"mse loss\")\n",
        "\n",
        "print(loss_h[:10])\n",
        "print(loss_h[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70il-LA_XGSO"
      },
      "source": [
        "\n",
        "## Optimiseur \n",
        "La descente de gradient représente en fait un code standard puisque les dérivées sont calculées automatiquement et que les variables sont idéntifiées.\n",
        "Pytorch inclut une classe très utile pour la descente de gradient, [torch.optim](https://pytorch.org/docs/stable/optim.html), qui permet :\n",
        "* d'économiser quelques lignes de codes\n",
        "* d'automatiser la mise-à-jour des paramètres \n",
        "* d'abstraire le type de descente de gradient utilisé (sgd,adam, rmsprop, ...)\n",
        "\n",
        "Une liste de paramètres à optimiser est passée à l'optimiseur lors de l'initialisation. La méthode **zero_grad()** permet de remettre le gradient à zéro et la méthode **step()** permet de faire une mise-à-jour des paramètres.\n",
        "\n",
        "Un exemple de code  utilisant l'optimiseur est donné ci-dessous. Testez et comparez les résultats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58sP5ryLeP3Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "ccbe632f-fb00-496a-fbd7-097550ee1d06"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-c06eb415c335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflineaire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboston_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mboston_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"iteration : {i}, loss : {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ],
      "source": [
        "Xdim = boston_x.size(1)\n",
        "\n",
        "w = torch.randn(1,Xdim,dtype=torch.float,requires_grad=True)\n",
        "b = torch.randn(1,dtype=torch.float,requires_grad=True)\n",
        "## on optimise selon w et b.  lr est le pas du gradient\n",
        "optim = torch.optim.SGD(params=[w,b],lr=EPS) \n",
        "for i in range(EPOCHS):\n",
        "  loss = MSE(flineaire(boston_x,w,b).view(-1,1),boston_y.view(-1,1))\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()  \n",
        "  if i % 100==0:  print(f\"iteration : {i}, loss : {loss}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1rxv3ychdhD"
      },
      "source": [
        "## C. Architecture modulaires & réseaux de neurones\n",
        "Dans le framework pytorch (et dans la plupart des frameworks analogues), le module est la brique de base qui permet de construire un réseau de neurones.  Il permet de représenter en particulier :\n",
        "* une couche du réseau (linéaire : **torch.nn.Linear**, convolution : **torch.nn.convXd**, ...)\n",
        "* une fonction d'activation (tanh : **torch.nn.Tanh**, sigmoïde : **torch.nn.Sigmoid** , ReLu : **torch.nn.ReLu**, ...)\n",
        "* une fonction de coût (MSE : **torch.nn.MSELoss**, L1 :  **torch.nn.L1Loss**, CrossEntropy binaire: **torch.BCE**, CrossEntropy : **torch.nn.CrossEntropyLoss**, ...)\n",
        "* mais également des outils de régularisation (BatchNorm : **torch.nn.BatchNorm1d**, Dropout : **torch.nn.Dropout**, ...)\n",
        "* un ensemble de modules : en termes informatique, un module est un conteneur abstrait qui peut contenir d'autres conteneurs) : plusieurs modules peuvent être mis ensemble afin de former un nouveau module plus complexe.\n",
        "\n",
        "\n",
        "Le fonctionnement est très proche des fonctions que nous avons vu ci-dessus (un module encapsule en fait une fonction de **torch.nn.Function**), mais de manière à gérer automatiquement les paramètres à apprendre. Un module est ainsi muni :\n",
        "* d'une méthode **forward** qui permet de calculer la sortie du module à partir des entrées\n",
        "* d'une méthode **backward** qui permet d'effectuer la rétro-propagation (localement).\n",
        "* tous les paramètres sont automatiquement ajoutés dans une liste interne, accessible par la fonction **.parameters()** du module.\n",
        "\n",
        "Ci-dessous un exemple de régression linéaire en utilisant les modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spguRLUjD60C"
      },
      "outputs": [],
      "source": [
        "EPOCHS=10\n",
        "## Création d'une couche linéaire de dimension Xdim->1\n",
        "net = torch.nn.Linear(Xdim, 1) \n",
        "## Passe forward du module :  équivalent à net.forward(x)[:10]\n",
        "print(\"Sortie du réseau\", net(boston_x)[:10])\n",
        "## affiche la liste des paramètres du modèle\n",
        "print(\"Paramètres et noms des paramètres\", list(zip(list(net.parameters()), list(net.named_parameters()))))\n",
        "\n",
        "## Création d'une fonction de loss aux moindres carrés\n",
        "mseloss = torch.nn.MSELoss()\n",
        "## on créé un optimiseur pour le réseau (paramètres w et b), avec un pas de gradient lr\n",
        "optim = torch.optim.SGD(params=net.parameters(),lr=EPS) \n",
        "# Juste pour info, ce n'est pas utile, les paramètres sont déjà initialisés.\n",
        "net.reset_parameters()\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    loss = mseloss(net(boston_x).view(-1,1),boston_y.view(-1,1))\n",
        "    print(f\"iteration : {i}, loss : {loss}\")\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxdRGqZtSlVt"
      },
      "source": [
        "## C.1. Création d'un réseau de neurones\n",
        "\n",
        "Avec ces briques élémentaires, il est très facile de définir un réseau de neurones standard :\n",
        "* soit en utilisant le conteneur **torch.nn.Sequential** qui permet d'enchaîner séquentiellement plusieurs modules\n",
        "* soit en définissant à la main un nouveau module.\n",
        "\n",
        "Ci-dessous un exemple  pour créer un réseau à deux couches linéaires avec une fonction d'activation tanh des deux manières différentes. Vous remarquez qu'il n'y a pas besoin de définir la méthode **backward**, celle-ci est héritée du conteneur abstrait et ne fait qu'appeler séquentiellement en ordre inverse les méthodes **backward** des différents modules. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c6I-PZRD-aN"
      },
      "outputs": [],
      "source": [
        "EPS = 1e-2\n",
        "EPOCHS=50\n",
        "\n",
        "#Réseau à la main (on le refera à la main derriere)\n",
        "class DeuxCouches(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DeuxCouches,self).__init__()\n",
        "    self.un = torch.nn.Linear(Xdim,5)\n",
        "    self.act = torch.nn.Tanh()\n",
        "    self.deux = torch.nn.Linear(5,1)\n",
        "  def forward(self,x):\n",
        "    return self.deux(self.act(self.un(x)))\n",
        "\n",
        "netDeuxCouches = DeuxCouches()\n",
        "\n",
        "mseloss = torch.nn.MSELoss()\n",
        "    \n",
        "optim = torch.optim.SGD(params=netDeuxCouches.parameters(),lr=EPS)\n",
        "for i in range(EPOCHS):\n",
        "    loss = mseloss(netDeuxCouches(boston_x),boston_y.view(-1,1))\n",
        "    print(f\"iteration : {i}, loss : {loss}\")\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlxBl_ni-UVh"
      },
      "outputs": [],
      "source": [
        "#Création d'un réseau à 1 couche cachée avec le module séquentiel (remplace l'objet précédent)\n",
        "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
        "\n",
        "mseloss = torch.nn.MSELoss()\n",
        "    \n",
        "optim = torch.optim.SGD(params=netSeq.parameters(),lr=EPS) # extraction auto des paramètres :)\n",
        "for i in range(EPOCHS):\n",
        "    loss = mseloss(netSeq(boston_x),boston_y.view(-1,1))\n",
        "    print(f\"iteration : {i}, loss : {loss}\")\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGIDLqItECDu"
      },
      "source": [
        "##  C.2. Méthodologie expérimentale et boîte à outils\n",
        "Pytorch dispose d'un ensemble d'outils qui permettent de simplifier les démarches expérimentales. Nous allons voir en particulier : \n",
        "* le DataLoader qui permet de gérer le chargement de données, le partitionement et la constitution d'ensembles de test et d'apprentissage; \n",
        "* le checkpointing qui permet de sauvegarder/charger les modèles en cours d'entraînement.\n",
        "* le TensorBoard (qui vient de tensorflow) qui permet de suivre l'évolution en apprentissage de vos modèles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ4MoJP4k4i0"
      },
      "source": [
        "### C.2.1 DataLoader\n",
        "Le <a href=https://pytorch.org/docs/stable/data.html>**DataLoader**</a> et la classe associée <a href=https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset> **Dataset**</a>  permettent en particulier de :\n",
        "* charger des données\n",
        "* pré-processer les données\n",
        "* de gérer les mini-batchs (sous-ensembles sur lequel on effectue une descente de gradient).\n",
        "\n",
        "La classe **Dataset** est une classe abstraite qui nécessite l'implémentation que d'une seule méthode, ```__getitem__(self,index)``` : elle renvoie le i-ème objet du jeu de données (généralement un couple *(exemple,label)*. \n",
        "\n",
        "La classe **TensorDataset** est l'instanciation la plus courante d'un **Dataset**, elle permet de créer un objet **Dataset** à partir d'une liste de tenseurs qui renvoie pour un index $i$ donné le tuple contenant les $i$-èmes ligne de chaque tenseur.\n",
        "\n",
        "La classe **DataLoader** permet essentiellement de randomiser et de constituer des mini-batchs de façon simple à partir d'une instance de **Dataset**. Chaque mini-batch est constitué d'exemples tirés aléatoirement dans le **Dataset** passé en paramètre et mis bout à bout dans des tenseurs. La méthode ```collate_fn(*args)``` est utilisée pour cela (nous verrons une customization de cette fonction dans une séance ultérieure). C'est ce générateur qui est généralement parcouru lors de l'apprentissage à chaque itération d'optimisation.\n",
        "\n",
        "Voici un exemple de code pour utiliser le DataLoader : \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZaWAFO8k8ze"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "\n",
        "## Création d'un dataset à partir des deux tenseurs d'exemples et de labels\n",
        "train_data = TensorDataset(boston_x,boston_y)\n",
        "## On peut indexer et connaitre la longueur d'un dataset\n",
        "print(\"DATASET:\\n\",len(train_data),train_data[5])\n",
        "\n",
        "## Création d'un DataLoader\n",
        "## tailles de mini-batch de 16, shuffle=True permet de mélanger les exemples\n",
        "# loader est un itérateur sur les mini-batchs des données\n",
        "loader = DataLoader(train_data, batch_size=16,shuffle=True ) # n'hésitez pas à jouer avec les paramètres\n",
        "\n",
        "#Premier batch (aléatoire) du dataloader : (nb batch = len/batch_size)\n",
        "print(\"DATA LOADER:\\n\",len(iter(loader)),\"\\n\",next(iter(loader))[0].size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLZ4ptt0-UVj"
      },
      "outputs": [],
      "source": [
        "\n",
        "EPS=1e-4\n",
        "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
        "optim = torch.optim.SGD(params=netSeq.parameters(),lr=EPS)\n",
        "\n",
        "# La boucle d'apprentissage :\n",
        "for i in range(EPOCHS):\n",
        "    cumloss = 0\n",
        "    # On parcourt tous les exemples par batch de 16 (paramètre batch_size de DataLoader)\n",
        "    for bx,by in loader:\n",
        "        loss = mseloss(netSeq(bx).view(-1),by)\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        cumloss += loss.item() # item pour un scalaire (sinon .data ou detach)\n",
        "    print(f\"iteration : {i}, loss : {cumloss/len(loader)}\") # loss sur un batch => diviser pour avoir une grandeur interprétable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9x2LC_6lCQm"
      },
      "source": [
        "### C.2.2 Checkpointing\n",
        "Les modèles Deep sont généralement long à apprendre. Afin de ne pas perdre des résultats en cours de calcul, il est fortement recommander de faire du **checkpointing**, c'est-à-dire d'enregistrer des points d'étapes du modèle en cours d'apprentissage pour pouvoir reprendre à n'importe quel moment l'apprentissage du modèle en cas de problème.  Il s'agit en pratique de sauvegarder l'état du modèle et de l'optimisateur (et de tout autre objet qui peut servir lors de l'apprentissage) toutes les n itérations. Toutes les variables d'intérêt sont en général disponibles par la méthode **state_dict()** des modèles et de l'optimiseur. \n",
        "\n",
        "En pratique, vous pouvez utilisé un code dérivé de celui ci-dessous.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URQTq8hrPJO0"
      },
      "outputs": [],
      "source": [
        "# Il existe différentes solutions: en voici une\n",
        "# mais ça marche\n",
        "# \n",
        "import os\n",
        "\n",
        "def save_state(epoch,model,optim,fichier):\n",
        "      \"\"\" sauvegarde du modèle et de l'état de l'optimiseur dans fichier \"\"\"\n",
        "      state = {'epoch' : epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
        "      torch.save(state,fichier) # pas besoin de passer par pickle\n",
        " \n",
        "def load_state(fichier,model,optim):\n",
        "      \"\"\" Si le fichier existe, on charge le modèle et l'optimiseur \"\"\"\n",
        "      epoch = 0\n",
        "      if os.path.isfile(fichier):\n",
        "          state = torch.load(fichier)\n",
        "          model.load_state_dict(state['model_state'])\n",
        "          optim.load_state_dict(state['optim_state'])\n",
        "          epoch = state['epoch']\n",
        "      return epoch\n",
        " \n",
        "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
        "optim = torch.optim.SGD(params=netSeq.parameters(),lr=EPS) # extraction auto des paramètres\n",
        "fichier = \"/tmp/netSeq.pth\"\n",
        "start_epoch = load_state(fichier,netSeq,optim)\n",
        "for epoch in range(start_epoch,EPOCHS):\n",
        "    cumloss = 0\n",
        "    for bx,by in loader:\n",
        "        loss = mseloss(netSeq(bx).view(-1),by)\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        cumloss += loss.item()\n",
        "    if epoch % 10 ==0: save_state(epoch,netSeq,optim,fichier)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IstQCvKblSvT"
      },
      "source": [
        "\n",
        "### C.2.3 GPU \n",
        "Afin d'utiliser un GPU lors des calculs, il est nécessaire de transférer les données et le modèle sur le GPU par l'intermédiaire de la fonction **to(device)** des tenseurs et des modules.  Il est impossible de faire une opération lorsqu'une partie des tenseurs sont sur GPU et l'autre sur CPU. Il faut que tous les tenseurs et paramètres soient sur le même device ! On doit donc s'assurer que le modèle, les exemples et les labels sont sur GPU pour faire les opérations.\n",
        "\n",
        "Par ailleurs, on peut connaître le device sur lequel est chargé un tenseur par l'intermédiaire de ```.device``` (mais pas pour un modèle, il faut aller voir les paramètres dans ce cas).\n",
        "\n",
        "Une manière simple d'utiliser un GPU quand il existe et donc d'avoir un code agnostique est la suivante : \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs8s7EwwlWTn"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "## On charge le modèle sur GPU\n",
        "## A faire avant la déclaration de l'optimiseur, sinon les paramètres optimisés ne seront pas les mêmes! \n",
        "## model =  model.to(device) \n",
        "loader = DataLoader(TensorDataset(boston_x,boston_y), batch_size=16,shuffle=True ) \n",
        "\n",
        "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
        "netSeq = netSeq.to(device)\n",
        "optim = torch.optim.SGD(params=netSeq.parameters(),lr=EPS)\n",
        "\n",
        "for i,(bx,by) in enumerate(loader):\n",
        "    ## On charge le batch sur GPU\n",
        "    bx, by = bx.to(device), by.to(device)\n",
        "    loss = mseloss(netSeq(bx).view(-1),by)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    if i % 10 ==0: print(\"batch \",i)\n",
        "\n",
        "\n",
        "print(\"Device du mini-batch : \", bx.device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5J1b55_lFR-"
      },
      "source": [
        "\n",
        "### C.2.4 TensorBoard\n",
        "\n",
        "Durant l'apprentissage de vos modèles, il est agréable de visualiser de quelle manière évolue le coût, la précision sur l'ensemble de validation ainsi que d'autres éléments. TensorFlow dispose d'un outil très apprécié, le TensorBoard, qui permet de gérer très facilement de tels affichages. On retrouve tensorboard dans **Pytorch** dans ```torch.utils.ensorboard``` qui permet de faire le pont de pytorch vers cet outil. \n",
        "\n",
        "Le principe est le suivant :\n",
        "* tensorboard fait tourner en fait un serveur web local qui va lire les fichiers de log dans un répertoire local. L'affichage se fait dans votre navigateur à partir d'un lien fourni lors du lancement de tensorboard.\n",
        "* Les éléments que vous souhaitez visualiser (scalaire, graphes, distributions, histogrammes) sont écrits dans le fichier de log à partir d'un objet **SummaryWriter** .\n",
        "* la méthode ```add_scalar(tag, valeur, global_step)``` permet de logger une valeur à un step donné, ```add_scalar(tag, tag_scalar_dic, global_step)``` un ensemble de valeurs par l'intermédiaire du dictionnaire ```tag_scalar_dic``` (un regroupement des scalaires est fait en fonction du tag passé, chaque sous-tag séparé par un **/**).\n",
        "\n",
        "Il existe d'autres méthodes ```add_XXX``` pour visualiser par exemple des images, des histogrammes (cf <a href=https://pytorch.org/docs/stable/tensorboard.html>la doc </a>).\n",
        "\n",
        "Le code suivant illustre une manière de l'utiliser. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kIhHDnElQd8"
      },
      "outputs": [],
      "source": [
        "# Spécial notebook, les commandes suivantes permettent de lancer tensorboard\n",
        "# En dehors du notebook, il faut le lancer à la main dans le shell : \n",
        "# tensorboard --logdir logs\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /tmp/logs\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# Spécial notebook : pour avoir les courbes qui s'affichent dans le notebook, \n",
        "# sinon aller à l'adresse web local indiquée lors du lancement de tensorboard\n",
        "from tensorboard import notebook\n",
        "notebook.display() # A voir si vous avez une autre fenêtre de gestion de tensorboard ou si vous le voulez à la suite\n",
        "\n",
        "EPS = 1e-5\n",
        "EPOCHS=1000\n",
        "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
        "netDeuxCouches = DeuxCouches()\n",
        "netSeq.name = \"Sequentiel\" # nommer les modèles\n",
        "netDeuxCouches.name = \"DeuxCouches\"\n",
        "\n",
        "\n",
        "mseloss = torch.nn.MSELoss()\n",
        "for model in [netSeq, netDeuxCouches]:\n",
        "    ## Obtention d'un SummaryWriter\n",
        "    ## meme répertoire que la commande %tensorboard --logdir logs \n",
        "    summary = SummaryWriter(f\"/tmp/logs/test/{model.name}/\") # on peut ajouter un timestamp ou des paramètres\n",
        "\n",
        "    optim = torch.optim.SGD(params=model.parameters(),lr=EPS) \n",
        "    for i in range(EPOCHS):\n",
        "        cumloss = 0\n",
        "        for bx, by in loader:\n",
        "            loss = mseloss(model(boston_x),boston_y.view(-1,1))\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()  \n",
        "            cumloss+= loss.item()\n",
        "        summary.add_scalar(f\"loss\",cumloss,i) # c'est ici qu'on fait le lien\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaW5Av4elaBN"
      },
      "source": [
        "# D. Exemple typique de code complet & applications\n",
        "* Le graphe de calcul est instancié de manière dynamique sous pytorch, et cela consomme des ressources. Lorsqu'il n'y a pas de rétropropagation qui intervient - lors de l'évaluation d'un modèle par exemple -, il faut à tout prix éviter de le calculer. L'environnement **torch.no_grad()** permet de désactiver temporairement l'instanciation du graphe. **Toutes les procédures d'évaluation doivent se faire dans cet environnement afin d'économiser du temps !**\n",
        "* Pour certains modules, le comportement est différent entre l'évaluation et l'apprentissage (pour le dropout ou la batchnormalisation par exemple, ou pour les RNNs). Afin d'indiquer à pytorch dans quelle phase on se situe, deux méthodes sont disponibles dans la classe module,  **.train()** et **.eval()** qui permettent de basculer entre les deux environnements.\n",
        "\n",
        "Les deux fonctionalités sont très différentes : **no_grad** agit au niveau du graphe de calcul et désactive sa construction (comme si les variables avaient leur propriété **requires_grad** à False), alors que **eval/train** agissent au niveau du module et influence le comportement du module.\n",
        "\n",
        "Vous trouverez ci-dessous un exemple typique de code pytorch qui reprend l'ensemble des éléments de ce tutoriel. Vous êtes prêt maintenant à expérimenter la puissance de ce framework.\n",
        "\n",
        "## D.1. Exemple complet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3TRg2p5ldCJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "import os\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /tmp/logs\n",
        "\n",
        "notebook.display()\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "def save_state(epoch,model,optim,fichier):\n",
        "    state = {'epoch' : epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
        "    torch.save(state,fichier)\n",
        "\n",
        "def load_state(fichier,model,optim):\n",
        "    epoch = 0\n",
        "    if os.path.isfile(fichier):\n",
        "        state = torch.load(fichier)\n",
        "        model.load_state_dict(state['model_state'])\n",
        "        optim.load_state_dict(state['optim_state'])\n",
        "        epoch = state['epoch']\n",
        "    return epoch\n",
        "\n",
        "\n",
        "    # Datasets\n",
        "from sklearn.datasets import load_boston\n",
        "boston = load_boston() ## chargement des données\n",
        "all_data = torch.tensor(boston['data'],dtype=torch.float)\n",
        "all_labels = torch.tensor(boston['target'],dtype=torch.float)\n",
        "\n",
        "# Il est toujours bon de normaliser\n",
        "all_data = (all_data-all_data.mean(0))/all_data.std(0)\n",
        "all_labels = (all_labels-all_labels.mean())/all_labels.std()\n",
        "\n",
        "train_tensor_data = TensorDataset(all_data, all_labels)\n",
        "\n",
        "# Split en 80% apprentissage et 20% test\n",
        "train_size = int(0.8 * len(train_tensor_data))\n",
        "validate_size = len(train_tensor_data) - train_size\n",
        "train_data, valid_data = torch.utils.data.random_split(train_tensor_data, [train_size, validate_size])\n",
        "\n",
        "\n",
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_loader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "net = torch.nn.Sequential(torch.nn.Linear(all_data.size(1),5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
        "net.name = \"mon_premier_reseau\"\n",
        "CHECK_FILE = \"/tmp/mon_premier_reseau.chk\"\n",
        "net = net.to(device)\n",
        "MyLoss = torch.nn.MSELoss()\n",
        "optim = torch.optim.SGD(params=net.parameters(),lr=1e-5)\n",
        "\n",
        "start_epoch = load_state(CHECK_FILE,net,optim)\n",
        "\n",
        "# On créé un writer avec la date du modèle pour s'y retrouver\n",
        "summary = SummaryWriter(f\"/tmp/logs/model-{time.asctime()}\")\n",
        "for epoch in range(EPOCHS):\n",
        "    # Apprentissage\n",
        "    # .train() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
        "    net.train()\n",
        "    cumloss = 0\n",
        "    for xbatch, ybatch in train_loader:\n",
        "        xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
        "        outputs = net(xbatch)\n",
        "        loss = MyLoss(outputs.view(-1),ybatch)\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        cumloss += loss.item()\n",
        "    summary.add_scalar(\"loss/train loss\",  cumloss/len(train_loader),epoch)\n",
        "     \n",
        "    if epoch % 10 == 0: \n",
        "        save_state(epoch,net,optim,CHECK_FILE)\n",
        "        # Validation\n",
        "        # .eval() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            cumloss = 0\n",
        "            for xbatch, ybatch in valid_loader:\n",
        "                xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
        "                outputs = net(xbatch)\n",
        "            cumloss += MyLoss(outputs.view(-1),ybatch).item()\n",
        "        summary.add_scalar(\"loss/validation loss\", cumloss/len(valid_loader) ,epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HujGOuB9lte2"
      },
      "source": [
        "## D.2. Jeu de données MNIST\n",
        "Ce jeu de données est l'équivalent du *Hello world* en programmation. Chaque donnée est un chiffre manuscrit (de 0 à 9). Les lignes suivantes vous permettent de charger le jeu de données.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH_GScQbltD0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "import os\n",
        "from tensorboard import notebook\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "root = './data'\n",
        "if not os.path.exists(root):\n",
        "    os.mkdir(root)\n",
        "\n",
        "# Téléchargement des données\n",
        "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.,), (1.0,))])\n",
        "# if not exist, download mnist dataset\n",
        "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
        "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzzVcYHJ-UVp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dimension of images (flattened)\n",
        "HEIGHT,WIDTH = train_set[0][0].shape[1],train_set[0][0].shape[2] # taille de l'image\n",
        "INPUT_DIM = HEIGHT * WIDTH\n",
        "\n",
        "#On utilise un DataLoader pour faciliter les manipulations, on fixe arbitrairement la taille du mini batch à 32\n",
        "all_train_loader = DataLoader(train_set,batch_size=32,shuffle=True)\n",
        "all_test_loader = DataLoader(test_set,batch_size=32,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alloOoFulri7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "## Affichage de quelques chiffres\n",
        "ex,lab = next(iter(all_train_loader))\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(ex[i].view(WIDTH,HEIGHT), cmap='gray', interpolation='none')\n",
        "  plt.title(\"Label : {}\".format(lab[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  ax = plt.gca()\n",
        "  ax.set_facecolor('white')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjPoDcEj28uk"
      },
      "source": [
        "##  D.3. <span class=\"alert-success\"> Exercice : Classification multi-labels, nombre de couche de couches, fonction de coût </span>\n",
        "\n",
        "L'objectif est de classer chaque image parmi les 10 chiffres qu'ils représentent. Le réseau aura donc 10 sorties, une par classe, chacune représentant la probabilité d'appartenance à chaque classe. Pour garantir une distribution de probabilité en sortie, il faut utiliser le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html> **Softmax** </a> : $$Sotfmax(\\mathbf{x}) = \\frac{\\exp{x_i}}{\\sum_{i=1^d} x_i}$$ qui permet de normaliser le vecteur de sortie.\n",
        "\n",
        "* Faites quelques exemples de réseau à 1, 2, 3 couches et en faisant varier les nombre de neurones par couche. Utilisez un coût moindre carré dans un premier temps. Pour superviser ce coût, on doit construire le vecteur one-hot correspondant à la classe : un vecteur qui ne contient que des 0 sauf à l'index de la classe qui contient un 1 (utilisez ```torch.nn.functional.one_hot```).  Comparez les courbes de coût et d'erreurs en apprentissage et en test selon l'architecture.\n",
        "* Le coût privilégié en multi-classe est la *cross-entropy**. Ce coût représente la négative log-vraisemblance : $$NNL(y,\\mathbf{x}) = -x_{y}$$ en notant $y$ l'indice de la classe et $\\mathbf{x}$ le vecteur de log-probabilité inféré. On peut utiliser soit son implémentation par le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss>**NLLLoss**</a>, soit - plus pratique - le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>**CrossEntropyLoss** <a>  qui combine un *logSoftmax* et la cross entropie, ce qui évite d'avoir à ajouter un module de *Softmax* en sortie du réseau. Utilisez ce dernier coût et observez les changements.\n",
        "* Changez la fonction d'activation en une ReLU et observez l'effet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38Q58_1b-tQs",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn.functional import one_hot\n",
        "from tqdm import tqdm\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "notebook.display()\n",
        "\n",
        "## On utilise qu'une partie du training test pour mettre en évidence le sur-apprentissage\n",
        "TRAIN_RATIO = 0.01\n",
        "train_length = int(len(train_set)*TRAIN_RATIO)\n",
        "ds_train, ds_test = random_split(train_set, (train_length, len(train_set)- train_length))\n",
        "\n",
        "#On utilise un DataLoader pour faciliter les manipulations, on fixe  la taille du mini batch à 300\n",
        "train_loader = DataLoader(ds_train,batch_size=300,shuffle=True)\n",
        "test_loader = DataLoader(ds_test,batch_size=300,shuffle=False)\n",
        "\n",
        "\n",
        "def accuracy(yhat,y):\n",
        "    # si  y encode les indexes\n",
        "    if len(y.shape)==1 or y.size(1)==1:\n",
        "        return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).double().mean()\n",
        "    # si y est encodé en onehot\n",
        "    return (torch.argmax(yhat,1).view(-1) == torch.argmax(y,1).view(-1)).double().mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQRDaFO6-UVr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# On construit 4 réseaux à tester\n",
        "# 10 sorties pour chaque réseau, une par classe. \n",
        "# Comme on va utiliser une cross entropy loss, on ne rescale pas les sorties (la cross entropy combine un softmax + NLLloss)\n",
        "# On pourrait utiliser une BCE loss (vu qu'on est dans un cas binaire pour chaque sortie), dans ce cas il faudrait ajouter une sigmoide en derniere couche.\n",
        "\n",
        "##  TODO "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em35Yvlb-UVr"
      },
      "source": [
        "## D.4.  <span class=\"alert-success\"> Exercice : Régularisation des réseaux </span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfrBCe_F-UVr"
      },
      "source": [
        "### Pénalisation des couches\n",
        "Une première technique pour éviter le sur-apprentissage est de régulariser chaque couche par une pénalisation sur les poids, i.e. de favoriser des poids faibles. On parle de pénalisation L1 lorsque la pénalité est de la forme $\\|W\\|_1$ et L2 lorsque la norme L2 est utilisée : $\\|W\\|_2^2$. En pratique, cela consiste à rajouter à la fonction de coût globale du réseau un terme en $\\lambda Pen(W)$ pour les paramètres de chaque couche que l'on veut régulariser (cf code ci-dessous).\n",
        "\n",
        "Expérimentez avec une norme L2 dans $\\{0,10^{-5},10^{-4},10^{-3},10^{-2},\\}$, observez les histogrammes de la distribution des poids et l'évolution de la pénalisation et du coût en fonction du nombre d'époques. Utilisez pour cela  un réseau à 3 couches chacune de taille 100 et un coût de CrossEntropy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plXOaVo4-UVs"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_l2(model,epochs,l2):\n",
        "    writer = SummaryWriter(f\"/tmp/logs/l2-{l2}-{model.name}\")\n",
        "    optim = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "    model = model.to(device)\n",
        "    print(f\"running {model.name}-{l2}\")\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        cumloss, cumacc, count = 0, 0, 0\n",
        "        for x,y in train_loader:\n",
        "            optim.zero_grad()\n",
        "            x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
        "            yhat = model(x)\n",
        "            l = loss(yhat,y)\n",
        "            # Ajout d'une pénalisation L2 sur toutes les couches\n",
        "            l2_loss = 0.\n",
        "            for name, value in model.named_parameters():\n",
        "                if name.endswith(\".weight\"):\n",
        "                    l2_loss += (value ** 2).sum()\n",
        "            l += l2*l2_loss\n",
        "            l.backward()\n",
        "            optim.step()\n",
        "            cumloss += l*len(x)\n",
        "            cumacc += accuracy(yhat,y)*len(x)\n",
        "            count += len(x)\n",
        "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
        "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
        "        writer.add_scalar('loss/l2',l2_loss,epoch)\n",
        "        if epoch % 50 == 0:\n",
        "            with torch.no_grad():\n",
        "                cumloss, cumacc, count = 0, 0, 0\n",
        "                for x,y in test_loader:\n",
        "                    x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
        "                    yhat = model(x)\n",
        "                    cumloss += loss(yhat,y)*len(x)\n",
        "                    cumacc += accuracy(yhat,y)*len(x)\n",
        "                    count += len(x)\n",
        "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
        "                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n",
        "                ix = 0\n",
        "                for module in model.layers:\n",
        "                    if isinstance(module, nn.Linear):\n",
        "                        writer.add_histogram(f'linear/{ix}/weight',module.weight, epoch)\n",
        "                        ix += 1\n",
        "\n",
        "##  TODO "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qk2al6h-UVt"
      },
      "source": [
        "### Dropout\n",
        "\n",
        "Une autre technique très utilisée est le <a href=https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html> **Dropout** </a>. L’idée du Dropout est proche du moyennage de modèle : en entraînant k modèles de manière indépendante, on réduit la variance du modèle. Entraîner k modèles présente un surcoût non négligeable, et l’intérêt du Dropout est de réduire la complexité mémoire/temps de calcul. Le Dropout consiste à chaque itération à *geler* certains neurones aléatoirement dans le réseau en fixant leur sortie à zéro. Cela a pour conséquence de rendre plus robuste le réseau.\n",
        "\n",
        "Le comportement du réseau est donc différent en apprentissage et en inférence. Il est obligatoire d'utiliser ```model.train()``` et ```model.eval()``` pour différencier les comportements.\n",
        "Testez sur quelques réseaux pour voir l'effet du dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQsK7jHuxKqM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_dropout(model,epochs):\n",
        "    writer = SummaryWriter(f\"/tmp/logs/{model.name}\")\n",
        "    optim = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "    model = model.to(device)\n",
        "    print(f\"running {model.name}\")\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        cumloss, cumacc, count = 0, 0, 0\n",
        "        model.train()\n",
        "        for x,y in train_loader:\n",
        "            optim.zero_grad()\n",
        "            x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
        "            yhat = model(x)\n",
        "            l = loss(yhat,y)\n",
        "            l.backward()\n",
        "            optim.step()\n",
        "            cumloss += l*len(x)\n",
        "            cumacc += accuracy(yhat,y)*len(x)\n",
        "            count += len(x)\n",
        "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
        "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
        "        if epoch % 50 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                cumloss, cumacc, count = 0, 0, 0\n",
        "                for x,y in test_loader:\n",
        "                    x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
        "                    yhat = model(x)\n",
        "                    cumloss += loss(yhat,y)*len(x)\n",
        "                    cumacc += accuracy(yhat,y)*len(x)\n",
        "                    count += len(x)\n",
        "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
        "                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n",
        "\n",
        "\n",
        "def get_dropout_net(in_features,out_features,dims,dropout):\n",
        "    layers = []\n",
        "    dim = in_features\n",
        "    \n",
        "    for newdim in dims:\n",
        "        layers.append(nn.Linear(dim, newdim))\n",
        "        dim = newdim\n",
        "        if dropout>0: layers.append(nn.Dropout(dropout))\n",
        "        layers.append(nn.ReLU())\n",
        "        dim = newdim\n",
        "    layers.append(nn.Linear(dim,out_features))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "##  TODO "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBOU7OJS-UVv"
      },
      "source": [
        "### BatchNorm\n",
        "\n",
        "On sait que les données centrées réduites permettent un apprentissage plus rapide et stable d’un modèle ; bien qu’on puisse faire en sorte que les données en entrées soient centrées réduites, cela est plus délicat pour les couches internes d’un réseau de neurones. La technique de <a href=https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html> **BatchNorm**</a> consiste à ajouter une couche qui a pour but de centrer/réduire les données en utilisant une moyenne/variance glissante (en inférence) et les statistiques du batch (en\n",
        "apprentissage).\n",
        "\n",
        "Tout comme pour le dropout, il est nécessaire d'utiliser ```model.train()``` et ```model.eval()```. \n",
        "Expérimentez la batchnorm. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhotZEi5-UVv"
      },
      "outputs": [],
      "source": [
        "def get_batchnorm_net(in_features,out_features,dims):\n",
        "    layers = []\n",
        "    dim = in_features\n",
        "    for newdim in dims:\n",
        "        layers.append(nn.Linear(dim, newdim))\n",
        "        dim = newdim\n",
        "        layers.append(nn.BatchNorm1d(dim))\n",
        "        layers.append(nn.ReLU())\n",
        "        dim = newdim\n",
        "    layers.append(nn.Linear(dim,out_features))\n",
        "    return nn.Sequential(*layers)\n",
        "##  TODO "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E1ooDN1-UVw"
      },
      "source": [
        "# Construction du sujet à partir de la correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrKVhTlz-UVw"
      },
      "outputs": [],
      "source": [
        "###  TODO )\",\" TODO \",\\\n",
        "    txt, flags=re.DOTALL))\n",
        "f2.close()\n",
        "\n",
        "### </CORRECTION> ###"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}